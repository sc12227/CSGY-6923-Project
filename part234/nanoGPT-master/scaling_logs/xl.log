# config/train_abc_char_xl.py from config.train_abc_char_base import * out_dir = 'out-abc-char-xl' n_layer = 12 n_head = 10 n_embd = 640 dropout = 0.1 tokens per iteration will be: 65,536 found vocab_size = 99 (inside data/abc_char/meta.pkl) Initializing a new model from scratch number of parameters: 59.06M /root/autodl-tmp/nanoGPT-master/train.py:196: FutureWarning: torch.cuda.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler('cuda', args...) instead. scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16')) num decayed parameter tensors: 50, with 59,373,440 parameters num non-decayed parameter tensors: 25, with 16,000 parameters using fused AdamW: True compiling the model... (takes a ~minute) step 0: train loss 4.6246, val loss 4.6195 iter 0: loss 4.6450, time 31057.03ms, mfu -100.00% iter 10: loss 2.6586, time 246.52ms, mfu 34.22% iter 20: loss 2.0735, time 248.53ms, mfu 34.19% iter 30: loss 1.7310, time 248.27ms, mfu 34.17% iter 40: loss 1.7118, time 251.10ms, mfu 34.11% iter 50: loss 1.5887, time 251.66ms, mfu 34.05% iter 60: loss 1.4943, time 252.35ms, mfu 33.99% iter 70: loss 1.5240, time 253.33ms, mfu 33.92%iter 14130: loss 0.3002, time 253.44ms, mfu 32.24%
iter 14140: loss 0.3053, time 254.08ms, mfu 32.34%
iter 14150: loss 0.3672, time 254.76ms, mfu 32.42%
iter 14160: loss 0.2947, time 258.54ms, mfu 32.44%
iter 14170: loss 0.3239, time 260.06ms, mfu 32.44%
iter 14180: loss 0.3630, time 253.47ms, mfu 32.52%
iter 14190: loss 0.3161, time 255.02ms, mfu 32.58%
iter 14200: loss 0.4031, time 258.03ms, mfu 32.59%
iter 14210: loss 0.3308, time 255.19ms, mfu 32.63%
iter 14220: loss 0.4171, time 255.09ms, mfu 32.68%
iter 14230: loss 0.3523, time 254.68ms, mfu 32.72%
iter 14240: loss 0.3663, time 254.91ms, mfu 32.76%
iter 14250: loss 0.2661, time 255.06ms, mfu 32.79%
iter 14260: loss 0.2904, time 254.84ms, mfu 32.82%
iter 14270: loss 0.2967, time 254.93ms, mfu 32.85%
iter 14280: loss 0.3188, time 254.81ms, mfu 32.87%
iter 14290: loss 0.2392, time 258.38ms, mfu 32.85%
iter 14300: loss 0.3336, time 256.57ms, mfu 32.85%
iter 14310: loss 0.3106, time 255.21ms, mfu 32.87%
iter 14320: loss 0.3463, time 255.30ms, mfu 32.89%
iter 14330: loss 0.3656, time 255.62ms, mfu 32.90%
iter 14340: loss 0.3082, time 255.41ms, mfu 32.91%
iter 14350: loss 0.3620, time 256.55ms, mfu 32.91%
iter 14360: loss 0.3283, time 255.51ms, mfu 32.92%
iter 14370: loss 0.3414, time 254.90ms, mfu 32.94%
iter 14380: loss 0.3118, time 255.24ms, mfu 32.95%
iter 14390: loss 0.3226, time 255.15ms, mfu 32.96%
iter 14400: loss 0.3328, time 255.20ms, mfu 32.97%
iter 14410: loss 0.3246, time 255.26ms, mfu 32.98%
iter 14420: loss 0.2694, time 255.17ms, mfu 32.98%
iter 14430: loss 0.3329, time 256.70ms, mfu 32.97%
iter 14440: loss 0.3556, time 256.96ms, mfu 32.96%
iter 14450: loss 0.3575, time 255.57ms, mfu 32.96%
iter 14460: loss 0.3015, time 255.05ms, mfu 32.97%
iter 14470: loss 0.3813, time 255.28ms, mfu 32.98%
iter 14480: loss 0.3012, time 255.63ms, mfu 32.98%
iter 14490: loss 0.3458, time 255.25ms, mfu 32.99%
step 14500: train loss 0.3192, val loss 0.3253
saving checkpoint to out-abc-char-xl
iter 14500: loss 0.3646, time 9747.35ms, mfu 29.77%
iter 14510: loss 0.3626, time 254.73ms, mfu 30.11%
iter 14520: loss 0.2882, time 254.75ms, mfu 30.41%
iter 14530: loss 0.3821, time 255.23ms, mfu 30.67%
iter 14540: loss 0.3319, time 255.33ms, mfu 30.91%
iter 14550: loss 0.3484, time 254.90ms, mfu 31.13%
iter 14560: loss 0.3063, time 257.48ms, mfu 31.29%
iter 14570: loss 0.3429, time 255.27ms, mfu 31.47%
iter 14580: loss 0.3234, time 257.65ms, mfu 31.59%
iter 14590: loss 0.3891, time 256.89ms, mfu 31.72%
iter 14600: loss 0.2640, time 254.91ms, mfu 31.85%
iter 14610: loss 0.3066, time 252.48ms, mfu 32.01%
iter 14620: loss 0.2508, time 253.13ms, mfu 32.14%
iter 14630: loss 0.2899, time 253.08ms, mfu 32.26%
iter 14640: loss 0.3208, time 255.12ms, mfu 32.34%
iter 14650: loss 0.3324, time 255.37ms, mfu 32.41%
iter 14660: loss 0.3202, time 255.04ms, mfu 32.48%
iter 14670: loss 0.2934, time 255.23ms, mfu 32.53%
iter 14680: loss 0.2753, time 254.89ms, mfu 32.59%
iter 14690: loss 0.3098, time 254.87ms, mfu 32.64%
iter 14700: loss 0.2534, time 255.48ms, mfu 32.68%
iter 14710: loss 0.2906, time 255.38ms, mfu 32.71%
iter 14720: loss 0.3873, time 257.04ms, mfu 32.72%
iter 14730: loss 0.2823, time 251.59ms, mfu 32.80%
iter 14740: loss 0.3554, time 254.82ms, mfu 32.83%
iter 14750: loss 0.3509, time 249.73ms, mfu 32.93%
iter 14760: loss 0.2827, time 253.67ms, mfu 32.96%
iter 14770: loss 0.3889, time 258.83ms, mfu 32.92%
iter 14780: loss 0.3958, time 254.59ms, mfu 32.94%
iter 14790: loss 0.2853, time 256.47ms, mfu 32.94%
iter 14800: loss 0.2572, time 255.01ms, mfu 32.95%
iter 14810: loss 0.3914, time 254.87ms, mfu 32.97%
iter 14820: loss 0.3000, time 255.16ms, mfu 32.97%
iter 14830: loss 0.3642, time 254.95ms, mfu 32.99%
iter 14840: loss 0.3327, time 255.26ms, mfu 32.99%
iter 14850: loss 0.3488, time 254.95ms, mfu 33.00%
iter 14860: loss 0.3104, time 255.25ms, mfu 33.01%
iter 14870: loss 0.4102, time 254.95ms, mfu 33.01%
iter 14880: loss 0.3391, time 253.89ms, mfu 33.03%
iter 14890: loss 0.3526, time 255.44ms, mfu 33.03%
iter 14900: loss 0.3915, time 255.40ms, mfu 33.03%
iter 14910: loss 0.3344, time 254.83ms, mfu 33.04%
iter 14920: loss 0.3035, time 255.20ms, mfu 33.04%
iter 14930: loss 0.3246, time 255.08ms, mfu 33.04%
iter 14940: loss 0.3482, time 254.95ms, mfu 33.05%
iter 14950: loss 0.3375, time 261.56ms, mfu 32.97%
iter 14960: loss 0.3021, time 260.26ms, mfu 32.91%
iter 14970: loss 0.3867, time 258.37ms, mfu 32.88%
iter 14980: loss 0.3725, time 257.83ms, mfu 32.87%
iter 14990: loss 0.4682, time 257.02ms, mfu 32.86%
step 15000: train loss 0.3188, val loss 0.3211
saving checkpoint to out-abc-char-xl
iter 15000: loss 0.2961, time 9841.12ms, mfu 29.66%
root@autodl-container-38c543b634-01944a96:~/autodl-tmp/nanoGPT-master# 